%:
\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{mathrsfs}
\usetikzlibrary{positioning}


\usetikzlibrary{arrows}
\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{eg}{Example}
\newtheorem*{thm}{Theorem}
\newtheorem*{corol}{Corollary}
\newtheorem{ex}{Exercise}[section]
{\theoremstyle{plain}
\newtheorem*{rmk}{Remark}
\newtheorem*{rmks}{Remarks}
\newtheorem*{lt}{Last time}
}
\newtheorem*{lem}{Lemma}
\usepackage{color}
\usepackage{CJK}
\title{Path to Efficient AGI}
\author{Xiyu Zhai}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

This paper is the prelude of a series of papers that explore a new school of AI methodologies, which naturally incorporates with established AI methods like deep learning, and shall lead us towards \textbf{efficient} artificial general intelligence.

Here \textbf{efficiency} is the key because an inefficient AGI will take too long to appear or too expensive to be of important usage, although they could be helpful for entry level tasks. The meaning of efficiency is twofold:

\begin{enumerate}[(i)]
	\item Statistical efficiency. The ability to learn accurately from limited examples.
	\item Computational efficiency. The ability to perform computation in time and within resource boundary.
\end{enumerate}

Biological intelligence, especially that of human beings, border collies, huskies, etc, although being far more efficient than deep learning, is still far from perfection. We are already beaten by computer programs in terms of memorization or rule based computation. A prediction of this note is that, a near optimal AGI implementation over the architecture of CPUs and GPUs will be far superior than biological intelligence, let alone deep learning.

This paper is the blueprint of how to achieve efficient AGI, including

\begin{enumerate}[(i)]
	\item 
	\item 
	\item 
\end{enumerate}

It's going to be followed by

\begin{enumerate}[(i)]
	\item a paper on a new programming language called Husky, which satisfies the langauge requirments proposed;
	\item a paper on image classification based on ideas in section XX;
	\item a paper on image generation based on ideas in section XX;
\end{enumerate}

\section{Related Works}



\section{Theories Based on Turing Machines}

\subsection{Conventions}

For the matter of succinctness, we shall in this section restrict ourselves to Turing machine level when thinking of computation. This is of course far from reality, but it helps with illuminating the high level ideas.

By an $\textbf{NP}$ problem, we mean a decision problem together with the proof of that decision which can be verified in polynomial time.

Everything will be finite, i.e. representable in a Turing machine.

\subsection{NP Problems Arising from ML Problems}
\begin{center}
\begin{tikzpicture}[
roundnode/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=4mm},
squarednode/.style={rectangle, draw=orange!60, fill=yellow!5, very thick, minimum size=4mm},
]
%Nodes
\node[squarednode]      (ml)                              {$\text{ML}$};
\node[roundnode]        (np)       		[right=of ml] {$\text{NP}$};

%Lines
\draw[->] (ml.east) -- (np.west);
\end{tikzpicture}
\end{center}

We begin with the well-known fundamental (non-unique) conversion of an ML problem into an NP problem.

An ML problem is about finding a function $f$ $\mathcal{X}\to \mathcal{Y}$ such that $\mathscr{L}(f):=\mathbb{E}_{(X,Y)\sim \mathcal{P}}l(f(X), Y)$ is small enough where $\mathcal{P}$ is an unknown distribution over $\mathcal{X}\times Y$ and $l:\mathcal{Y}\times \mathcal{Y} \to \mathbb{R}$ is a loss function, and we are given sampling from the unknown distribution $\mathcal{P}$ in a certain fashion (online or offline).

A model is a class $\mathcal{H}$ of functions from $\mathcal{X}$ to $\mathcal{Y}$ (called Hypothesis Space). We say the model is good if one of $f\in \mathcal{H}$ will make $\mathscr{L}(f)$ small enough.

Suppose that we're given enough data (the amount is still polynomial, which is possible if $\log \mathcal{H}$ is polynomial, which will be true if elements in $\mathcal{H}$ arer polynomially presentable), then $\widehat{\mathscr{L}}(f):=todo$ becomes a good approximation of $\mathscr{L}(f)$. Then whether the model is a good one becomes an NP problem, with the proof being the specific $f\in \mathcal{H}$ making $\widehat{\mathscr{L}}(f)$ small enough.

More generally, we could think of meta learning. Suppose we have a family of machine learning problems indexed by S, todo
\subsection{ML Problems Arising from NP Problems}
\begin{center}
\begin{tikzpicture}[
roundnode/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=4mm},
squarednode/.style={rectangle, draw=orange!60, fill=yellow!5, very thick, minimum size=4mm},
]
%Nodes
\node[roundnode]        (np)       		              {$\text{NP}$};
\node[squarednode]      (ml)           [above=of np]  {$\text{ML}$};

%Lines
\draw[->] (np.north) -- (ml.south);
\end{tikzpicture}
\end{center}

Given an NP problem, which consists of an input space $\mathcal{X}$, a certificate space $\mathcal{C}$, and a polytime verifier $v: \mathcal{X} \rightarrow \text{Bool}$, the goal is then to find an efficient implementation or approximation of $x\mapsto \exists_{c\in \mathcal{C}} v(x,c)$.

Let's start with some brute force method for find certificates, denoted by $c_{\text{brute}}(x): \mathcal{X} \to \mathcal{C}$.

Suppose that we are satisfied with the approximation $x\mapsto v(x,c_{\text{brute}}(x))$ and we only want to make it faster. We can collect a set of $x_i\in \mathcal{X}$, then build a dataset with $y_i=c_{brute}(x)$, then this becomes a machine learning problem.

(A more appropriate setup would be to make $c_{brute}(x)$ being a small finite set of certificates, and $v(x,c_{\text{brute}}(x))$ will become $\exists c\in c_{\text{brute}}(x), v(x, c)$)

Now in general we can't be satisfied with the simple approximation $x\mapsto v(x,c_{\text{brute}}(x))$, we can also use machine learning to make a better approximation.

One way is to divide and conquer. Factor $\mathcal{C}$ into a disjoint union $\sqcup_{i\in \mathcal{I}} \mathcal{C}_{i}$ indexed by a not necessarily small set $\mathcal{I}$. Then
\begin{equation}
	f_*(x)=\exists i\in \mathcal{I}, \exists c\in \mathcal{C}_i, v(x,c).
\end{equation}

Suppose that we have a brute force method $c_{\text{brute},i}$ for each $\mathcal{C}_i$. Define
\begin{equation}
	g: \mathcal{X}\times \mathcal{I} \rightarrow \text{Bool}, g(x, i) = v(x, c_{\text{brute},i}(x))
\end{equation}

Now $g$ is a function we can approximate with machine learning. Let the approximation we get be $\hat{g}$, then we can simplify the problem by searching within $\mathcal{I}$ of $s$

\subsection{the Ladder of NP-ML Ascension}
\begin{center}
\begin{tikzpicture}[
roundnode/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=4mm},
squarednode/.style={rectangle, draw=orange!60, fill=yellow!5, very thick, minimum size=4mm},
]
%Nodes
\node[squarednode]      (ml0)                              {$\text{ML}_0$};
\node[roundnode]        (np1)       		[right=of ml0] {$\text{NP}_1$};
\node[squarednode]      (ml1)       		[above=of np1] {$\text{ML}_1$};
\node[roundnode]        (np2)       		[right=of ml1] {$\text{NP}_2$};
\node[squarednode]      (ml2)       		[above=of np2] {$\text{ML}_2$};
\node[roundnode]        (np3)       		[right=of ml2] {$\text{NP}_3$};
\node[squarednode]      (ml3)       		[above=of np3] {$\text{ML}_3$};

%Lines
\draw[->] (ml0.east) -- (np1.west);
\draw[->] (np1.north) -- (ml1.south);
\draw[->] (ml1.east) -- (np2.west);
\draw[->] (np2.north) -- (ml2.south);
\draw[->] (ml2.east) -- (np3.west);
\draw[->] (np3.north) -- (ml3.south);
\end{tikzpicture}
\end{center}

\subsection{Reinforcement Learning}

\section{Theories Based on Realistic Computation Models}



\section{Type System}

\subsection{Concept Level Types}


\subsection{System Level Types}

\subsection{Types for Machine Learning}

\section{Language Requirements}

\section{System}

\subsection{Database}

\subsection{Debugger}

\subsection{todo}

\end{document}